Google Article on Distributed Systems
========================================================

Leslie Lamport Quote
--------------------------------------------------------

:A distributed system is: one in which the failure of a computer that
        you didn't even know existed causes your computer to be unuseable

* No single point of failiure
        * -> many points of failure -> partial failiure
* more scalable, you can throw more hardware nodes at the problem
* shared computation between nodes can lead us to lots of problems

Tutorial
--------------------------------------------------------


:A distributed system is: an application that executes a collection
                          of protocols to co-ordinate the actions
                          of multuple processes on a network, such that
                          all components coperate together to perform a
                          single or small set of related tasks.

Fault tolerance is really important:

        :Fault Tolerant: the system can recover from failiures without 
                         fucking up something else

        :Highly Available: continues to provide services even when some
                components have failed

        :Recoverable: components can be added back into the system after
                they have been repaired, or the failiure has been otherwise 
                resolved

        :Consistent: the system acts like a non-distributed system by co-ordinating
                the actions of it's components while managing failures and concurrency

        :Scalable: continues to operate effectivley when the system is scaled in
                some aspect such as the number of nodes in the network or the overall
                load on the system. These things add complexity and increase the frequency
                of failures. This should not have a significant efffect on the system 
                as a whole

        :predictable performance: does the system behave predictably when 
                  more nodes are added? when more data is added?

Security is also really important


Distributed systems must be designed with the **expectation** of failiure

* If there is a 1/10,000 chance of a failiure, that means it happends **all the time**
* Distributed systems must be designed with the though that failiure happens 
  all the time, because it kind of does
* --> **multiplier on the value of simplicity**

Bugs
________________________________________________________


Two major categories of bugs in distributed systems:

        :Heisenbug: A bug that seems to change or disappear entirley
                when it is observed/researched (Heisenburg Uncertainty Principle)

        :Bohrbug: opposite of a Heisenbug. It manifests itself under a well defined 
                set of conditions (consistently)




Failiures and Fallacies
________________________________________________________


many types of failiure:

        :haulting failure: the thing just stops

                * can't tell if it's slow or dead
                * timeout goes off, but that doesnt mean it's shut down

        :Fail stop: like a haulting failure but with some notification
                to other components

        :Omission Failiures: a message didn't make it (primarily due to buffer overflow
                at some node in the network)

        :Network Failure: the network goes

        :Network Partition: there is a split between two or more disjoint
                clusters in the network. the network is operational within these
                sub-networks, but they cannot communicate accross their bounds

        :Timing: some temporal property of the system is violated
                no centralized clock --> dificulty with timing

        :Bysantine: data corruption, malicious programs, a whole list here

Some wrong assumptions can lead to bysantine behaviour
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


        * Reliable network
        * zero latency
        * infinite bandwidth
        * secure network
        * topologies don't change
                * truth is that nodes come and go
        * 1 administrator
        * Transport cost (copying, buffering)
        * Homogenous network (nodes/links are the same accross the system)


How?
________________________________________________________


:client-server: more realistically, service-client where the service is 
                made up of multiple running nodes

* binding
        - A process that wants to use a service gets associated to 
          a specific server providing the service, think :func:`socket.bind(addr, port)`
* replication
        - multiple copies of data across the system to increase availability
          (if some node goes down) and allow local access in multiple locations
* caching
        - store a local copy of the data for quick access if it is requested again 
        - cach data can go stale and needs to be updated periodically. If the cached 
          data is refreshed by/ syncronized to the service data then this is actually
          replication

TCP, UDP, **RPC**


:**RPC**: Remote Procedure Call, It's like a distributed function call

Remote Procedure Calls
________________________________________________________


emulates a local procedure call over a network. Arguments are sent to the 
server through a packet (or multiple). The server executes the requested service
and sends a response packet back to the client. The client process can then 
continue 



Design Principles
________________________________________________________


* idenentify failure senarios explicitly and ensure that the code is covered for
  the most likley ones
* prepare of unresponsive senders/receivers in both the client and server code
* minimize network traffic, every request/response increases the probability of failure
* don't assume that data arriving from one process to another is the same data that was
  sent. Use validity checks like checksums to ensure the data has not changed
* avoid acks, they're expensive
* retransmission is costly. Adjust the delay for optimal transmission


When a process  A stores information that cannot be reconstructed it can pose a problem to 
the distributed system. Other processes must go A for that data, A becomes a single point
of failiure. So A's data must be replicated somewhere else as a backup, but then we have 
problems with managing the changes of that data between the replicated sources. This
presents a trade-off between accessability of the data (the ability to procure it at all)
and acccuracy of the data.


:DONE: review forum posts with answers :done:8/05/20 06:18PM
:link: https://coursespaces.uvic.ca/mod/hsuforum/discuss.php?d=87144#p145524

:DONE: Answer some more question from the article :done:9/05/20 06:18PM
:link: http://www.hpcs.cs.tsukuba.ac.jp/~tatebe/lecture/h23/dsys/dsd-tutorial.html

.. include:: google_exercises.rst
