Distributed Systems for Fun and Profit
===========================================

:Question from prelab exercise:
        What do you find most interesting about Chapter 1 
        (Distributed systems at a high level), and why?

:My Answer:

        I found that Takada gave interesting explanations for many of the trade-offs 
        involved in distributed computing. 
        (Performance vs Availability, Intelligibility vs Performance, etc). 


        What was most interesting to me, from the first chapter, was the notion
        that oftentimes availability must be sacrificed in order 
        to maintain guarantees about latency. Thus 
        exposing the trade-off between availability & latency guarantees. 

        I also enjoyed the explanation at the beginning of the chapter 
        regarding the theoretical lack of need for distributed systems in 
        the first place. Takada expresses the notion that, given infinite r&d 
        resources there would be no requirement, or benefit, to splitting a 
        computation across multiple nodes. Instead the hardware of a single machine
        could be indefinitely upgraded. 

        The reason that distributed systems are required in practice is only due
        to the limitations of our current technology.  

Two consequences of distribution:

* Information travels at the speed of light
* **independent things fail independently**

Notes from Chapter 1
---------------------------------------

As system requirements for a computation increase, a point 
is reached at which it is no longer feasible (or is cost prohibitive)
to upgrade a single machine to do the job. In consequence of this 
it becomes a requirement to run computations over multiple machines 
(a distributed system).

:Definition of **Scalability**: 
        The ability of a system to be enlarged to accommodate
        a growing amount of work

In other words, as the size of the problem increases the performance 
should get **incrementally worse**.

:Size Scalability: adding more nodes to the problem should make the 
                   system **linearly** faster. Ideally 3x as many nodes
                   would mean 3x as fast performance, or 3x as much data
                   processed in the same amount of time.

:Geographic Scalability: possibility of adding more data centres in different 
                         locations to increase response times (dealing with 
                         cross-data centre latency reasonably).

:Admin Scalability: The ability to add nodes to the system without 
                    increasing administrative costs.


Performance & Availability 
_______________________________________

**Performance** is the amount of work accomplished by a system
in comparison to the resources (including time) used.

In contrast, **Availability** is the *proportion* of time in which 
the system functions as intended. 

.. math::

        Availability = \frac{uptime}{uptime + downtime}


Latency vs Latent
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:**Latency**:  The period of delay between the initiation of something
               and the occurrence of it.

               - example; how long does it take for a write to be visible 
                 to readers?

:Latent: Something is **latent** if it exists but is inactive.

Availability & Fault Tolerance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Fault Tolerance** is the ability of a system to
continue to behave (in a well-defined way) when faults occur.

.. note::

        You cannot tolerate faults which you have not considered

        ==> It is important to consider what can go wrong in a distributed 
        system and design the system appropriatley 

What's in the Way?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A distributed system is constrained by both the **number of nodes**
and the **distance between them**.

Thus the following is clear:

* more nodes => higher probability of failure
* more nodes => more communication overhead
* farther apart nodes => increased latency of communication

I use the notation '=>' above since these factors are 
universally true (tautology) as they are a consequence of 
**physical constraints**. 

Any other factors are a consequence of a system's **design**.


Intelligibility vs Efficiency 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Generally speaking, there is a trade off between 
how easy a design is to understand and it's performance. 

A system which makes **weak guarentees** has more freedom of action, 
and thus more potential for performance. However it also becomes 
more complicated to reason about.

When failures occur which increase the latency of a system, a system 
must decide whether do reduce availability, & thus maintain it's 
guarantees about performance, or to stay available and fail to meet
it's performance guarantees.

In this sense, there is a trade off between **availabilty** and **Performance**.

Technique: Partition & Replicate
_______________________________________

A Divide & Conquer approach

1. Partition any data that can be partitioned and provide one 
   partition to each node
2. Replicate any data which cannot be partitioned and provide 
   a copy of that data to each node

:A problem with replication:
        maintaining multiple copies of the same data requires some way to
        keep that data synchronised and/or deal with inconsistencies.

Furthermore, there is a trade off between the strength of consistency 
and latency. A weaker consistency model will be faster than a stronger one.


Chapter 2
--------------------------------------------------------------------

:Question from prelab: 
        What did you find most interesting in Chapter 2 of Distributed 
        Systems for Fun and Profit and why?

:My Answer:
I found that the Author gave some interesting thoughts about abstractions, and how they can 
often break down in distributed systems. The part of the chapter which 
I found most interesting was the explanations of the FLP Impossibility Result and the CAP theorem. 


In regards to the FLP Impossibility Result, I found it 
interesting how many assumptions must be made in providing a 
deterministic consensus algorithm. All that is required for a 
deterministic algorithm not to be possible is that a single process can 
fail. Thus, in order to guarantee a finite runtime, one must assume that there 
will be no failures at all. In practical applications it is unlikely that a consensus 
algorithm would not eventually halt. 
However it is interesting to know that there will never be a "perfect" consensus algorithm. 

The CAP theorem brings about some really interesting trade offs. Knowing that it is not 
possible to have strong consistency and complete availability at the same time during a 
network failure leads us to ask how loose of a consistency model we can get away with. 
If strong consistency is truly required for our system then what sacrifices can we make in regards to availability? 

I also liked the idea of having Client-centric consistency in which you don't
guarantee perfectly up to data data, but you do guarantee that no client will 
receive older data than they have already. By maintaining a cached value as part 
of the client library you can ensure that, on each request, the client will receive data 
which is at least as recent as what it has already received. Based on my experience I feel 
that this is sufficient not to break client code in most cases. Usually it is not critical 
to have the MOST up to date data all of the time. However, sometimes receiving data which is 
more out of date than what was previously received would definitely lead to some elusive bugs, 
especially if it happens infrequently and is thus not expected.



Abstractions are fake
__________________________________________________________________

Abstractions make the world more manageable, but fundamentally  every 
situation is at least a little different. Thus every node is a little 
different. Though we make abstractions such that we can treat each node 
as the same, we must understand that specific circumstances at any node
could break that abstraction at any time. 


How much reality can be stripped away without missing anything important? 
it is likely not possible to know in advance (the time of designing the abstraction).


A System Model
___________________________________________________________________

A system model is a set of assumptions about the environment on which 
a distributed system is implemented. 

*  What capabilities do the nodes have
        * How may they fail at those capabilities
* time? Order? How does that work?

...



The Consensus Problem
________________________________________________________________

:FLP Impossibility Result:
        There is no deterministic algorithm for the consensus problem 
        in an asynchronous system which is subject to failures. 

        - even with a reliable network
        - even if a single process can fail at a time
        - even if that process only fails by crashing

        Most of the time consensus can be reached, but 
        there is no guarantee that an algorithm will halt. 

:CAP theorem:
        It is not possible to have all three of these, you must pick 2. 

        - Consistency: same data at the same time
        - Availability: survivors continue to operate despite node failures
        - Partition Tolerance: system still operates despite message losses

        This separates D.systems into 3 categories

        - CA systems which have consistency and availability, but do not tolerate network partitions 

                * does not distinguish between node failures and network failures. 
                * must stop accepting writes in the case of any failure to remain consistent & available. 
                        * you cannot maintain two replicas without communicating while accepting writes. 
                          Thus the only way to remain consistent is to stop accepting writes all together. 

        - CP systems like PAXOS which do not remain available during partitions, but do remain consistent 
                
                * forces the smaller side of the partition to be unavailable, thus maintaining consistency 
                  in the majority partition and sacrificing availability of the minority partition.

        - AP systems like Dynamo which do not remain consistent during partitions, but do remain available 



In the event of a partition there are 3 options to choose from.

1. Stop accepting writes across the entire system. Thus maintaining consistency 
   and accessibility for reads, but otherwise halting progress.

2. Continue to provide strong consistency by making the minority partition unavailable.

3. Continue to provide availability by having less strict consistency requirements 
   (not requiring full consensus on every write). 


Consistency Models
______________________________________________________________


:Client-centric: guarantee that the client will not read data which is older than
                 what it has already seen. It may not be the "newest" data, but it will be 
                 updated (or the same) as their previous lookup.  Essentially, stop the client from 
                 travelling back in time in regards to their data. 
                 This is done by maintaining a cached 
                 value in the client library. 












